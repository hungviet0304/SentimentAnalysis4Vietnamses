{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SentimentAnalysisforVietnamese.ipynb.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"55KkFnGNRSe-","colab_type":"code","colab":{}},"source":["import numpy as np\n","from pyvi import ViTokenizer as vitoken\n","import os\n","import io\n","import re\n","import string\n","import json\n","from collections import Counter\n","\n","regex = '[^A-Za-zÁáÀàẢảÃãẠạĂăẮắẰằẲẳẴẵẶặÂâẤấẦầẨẩẪẫẬậÐđÉéÈèẺẻẼẽẸẹÊêẾếỀềỂểỄễỆệĨÍíÌìỈỉĨĩỊịÓóÒòỎỏÕõỌọÔôỐốỒồỔổỖỗỘộƠơỚớỜờỞởỠỡỢợÙùÚúỦủŨũỤụƯưỨứỪừỬửỮữỰựÝýỲỳỶỷỸỹỴỵ]+'  # only character and number\n","STOP_WORD_DIR = \"C:\\\\Users\\\\hungv\\\\OneDrive - VNU-HCMUS\\\\000@@@1.KL\\\\00002.data\\\\stop_word\"\n","MODEL_DIR = \"C:\\\\Users\\\\hungv\\\\OneDrive - VNU-HCMUS\\\\000@@@1.KL\\\\00003.models\"\n","data_dir = \"C:\\\\Users\\\\hungv\\\\OneDrive - VNU-HCMUS\\\\000@@@1.KL\\\\00002.data\"\n","TRAIN_FOLDER = 'data_train'\n","TEST_FOLDER = 'data_test'\n","VALIDATE_FOLDER = 'data_validation'\n","FILE_CLEANED_LABELED = \"file_cleaned_label.txt\"\n","WORD2INDEX = 'word2index.txt'\n","FINAL_FILE = \"shuffled_data.txt\"\n","VOCAB = 'vocab_new.txt'\n","FILE_CLEANED_LABELED_TRAIN = 'file_cleaned_label_train.txt'\n","# flags\n","TRAINING_DATA = 1\n","TESTING_DATA = 2\n","VALIDATION_DATA = 3\n","\n","\n","# vocabulary\n","def build_vocab(tokenized_data):\n","    # build vocab\n","    tmp_list = [item for sentence in tokenized_data for item in sentence]\n","    vocab = Counter()\n","    vocab.update(tmp_list)\n","    tokens = [k for k, c in vocab.items() if c >= 3]\n","\n","    f = open(os.path.join(data_dir, 'vocab_new.txt'), 'w', encoding='utf-8')\n","    for token in tokens:\n","        f.write(token + '\\n')\n","    f.close()\n","\n","\n","def load_vocab(file_name):\n","    file = open(file_name, 'r', encoding='utf-8')\n","    text = file.read()\n","    text = text.split('\\n')\n","    file.close()\n","    return text\n","\n","\n","def save_vocab(vocab, file_name):\n","    data = \"\\n\".join(vocab)\n","    file = open(file_name, 'w', encoding='utf-8')\n","    file.write(data)\n","    file.close()\n","\n","\n","def normalize_data(sentence: string):\n","    sentence = re.sub(\" +\", \" \", sentence)\n","    if sentence:\n","        sentence = re.sub(regex, ' ', sentence)\n","        sentence = sentence.lower()\n","        sentence = sentence.lstrip()\n","        if len(sentence) > 1:\n","            return sentence\n","        else:\n","            return False\n","\n","\n","def clean_data(data: list):\n","    '''\n","\n","    :param data: @data: untokenize data\n","    :return: cleaned data, tokenized\n","    '''\n","    cleaned_data = []\n","    vocab = load_vocab(os.path.join(data_dir, VOCAB))\n","    vocab = set(vocab)\n","\n","    for i in range(len(data)):\n","        if type(data[i]) is str:\n","            if len(data[i]) > 1:\n","                tmp_sentence = data[i].lower().replace(\"\\n\", \" \")\n","                tmp_sentence = normalize_data(tmp_sentence)\n","                # print(i)\n","                # if i == 9523:\n","\n","                if type(tmp_sentence) is str and len(tmp_sentence) > 1:\n","                    tmp_sentence = vitoken.tokenize(tmp_sentence)\n","                    print(tmp_sentence)\n","                    tmp_sentence = tmp_sentence.split(\" \")\n","                    print(tmp_sentence)\n","                    tmp_sentence = [w for w in tmp_sentence if w in vocab]\n","                    print(tmp_sentence)\n","                    cleaned_data.append(tmp_sentence)\n","    removed_stop_word = remove_stop_words(cleaned_data)\n","\n","    return removed_stop_word\n","\n","\n","def add_labels(positive_list: list, negative_list: list):\n","    link = os.path.join(data_dir, FILE_CLEANED_LABELED)\n","\n","    with open(link, 'w', encoding=\"utf-8\") as writer:\n","        for i in range(len(positive_list)):\n","            # sent = ' '.join(positive_list[i])\n","            # line = sent + '| ' + str(1) + \"\\n\"\n","\n","            writer.write(positive_list[i] + '| ' + str(1) + '\\n')\n","\n","        for i in range(len(negative_list)):\n","            # sent = ' '.join(negative_list[i])\n","            # line = sent + '| ' + str(-1) + \"\\n\"\n","            # writer.write(line)\n","            writer.write(negative_list[i] + '| ' + str(-1) + '\\n')\n","\n","        writer.close()\n","\n","\n","def remove_stop_words(tokenized_data: list):\n","    # remove all stop word\n","    stop_word_file = open(os.path.join(STOP_WORD_DIR, \"vietnamese-stopwords.txt\"), 'r', encoding='utf-8')\n","    stop_word_list = stop_word_file.read()\n","    stop_word_file.close()\n","    stop_word_list = stop_word_list.split('\\n')\n","    tokenized_sentences_stop_word = []\n","    for sentence in tokenized_data:\n","        index = tokenized_data.index(sentence)\n","        not_in_stop_word = []\n","        for word in sentence:\n","            if word not in stop_word_list:\n","                not_in_stop_word.append(word)\n","        if len(not_in_stop_word) > 1:\n","            tokenized_sentences_stop_word.append(not_in_stop_word)\n","    return tokenized_sentences_stop_word\n","\n","\n","def word2index(tokenized_data: list):\n","    word_2_index = {'PAD': 0}\n","    number_of_words = 1\n","    for i in range(len(tokenized_data)):\n","        for token in tokenized_data[i]:\n","            if token not in word_2_index:\n","                word_2_index[token] = number_of_words\n","                number_of_words += 1\n","\n","    with io.open(os.path.join(data_dir, WORD2INDEX), 'w', encoding='utf-8') as json_file:\n","        json.dump(word_2_index, json_file, ensure_ascii=False)\n","    return word_2_index\n","\n","\n","def pad_sequence(sequence, max_length):\n","    padded_seq = []\n","    for i in range(len(sequence)):\n","        padded_seq.append(sequence[i])\n","        print(i)\n","        k = len(sequence[i])\n","        while (k < max_length):\n","            padded_seq[i].append(0)\n","            k = len(padded_seq[i])\n","\n","    return sequence\n","\n","\n","def word_to_index(data: list, word2idx: dict):\n","    sequence = []\n","    for sentence in data:\n","        sentence = [word2idx.get(token) for token in sentence if word2idx.get(token) is not None]\n","        sequence.append(sentence)\n","    return sequence\n","\n","\n","def shuffle_file():\n","    '''Shuffle the data in the file '''\n","    link = os.path.join(data_dir, FILE_CLEANED_LABELED)\n","\n","    with open(link, 'r', encoding='utf-8') as source:\n","        data = [(np.random.random(), line) for line in source]\n","    data.sort()\n","    link = os.path.join(data_dir, FINAL_FILE)\n","\n","    with open(link, 'w', encoding='utf-8') as target:\n","        for _, line in data:\n","            target.write(line)\n","\n","\n","# def split_train_test_file():\n","#     link = os.path.join(data_dir, FINAL_FILE)\n","#     f = open(link, encoding='utf-8')\n","#     data = f.read()\n","#\n","#     n_len = len(data)\n","#     test_size = 0.2 * n_len\n","#     train_size = n_len - test_size\n","#     train_writer = open(TRAIN_FOLDER, 'w', encoding='utf-8')\n","#     test_writer = open(TEST_DATA, 'w', encoding='utf-8')\n","#\n","#     for i in range(n_len):\n","#         if i < test_size:\n","#             test_writer.write(data[i])\n","#         else:\n","#             train_writer.write(data[i])\n","\n","\n","def count_labels(labeled):\n","    pos = 0\n","    neg = 0\n","    neu = 0\n","    for label in labeled:\n","        if label == 1:\n","            pos += 1\n","        elif label == 0:\n","            neu += 1\n","        else:\n","            neg += 1\n","    print(str(pos) + \"- \" + str(neg) + \"- \" + str(neu))\n","\n","\n","def load_data_new_version():\n","    # path = os.path.join(data_dir, VALIDATE_FOLDER)\n","    # labels = os.listdir(path)\n","    #\n","    # neg_data = []\n","    # pos_data = []\n","    # for label in labels:\n","    #     link = os.path.join(path, label)\n","    #     list_files = os.listdir(link)\n","    #     for file in list_files:\n","    #         f = open(os.path.join(link, file), 'r', encoding='utf-8')\n","    #         sentence = f.read()\n","    #         sentence = sentence.replace('\\n', ' ')\n","    #         if label == 'neg':\n","    #             neg_data.append(sentence)\n","    #         else:\n","    #             pos_data.append(sentence)\n","    #\n","    # # d = pos_data + neg_data\n","    # pos_cleaned_data = clean_data(pos_data)\n","    # neg_cleaned_data = clean_data(neg_data)\n","    # print(\"pos: %d\" % len(pos_cleaned_data))\n","    # print(\"neg: %d\" % len(neg_cleaned_data))\n","    # # d = clean_data(d)\n","    # # word2idx = word2index(d)\n","    #\n","    # f = open(os.path.join(data_dir, \"positive_cleaned.txt\"), 'w', encoding='utf-8')\n","    # for sentence in pos_cleaned_data:\n","    #     line = ' '.join(sentence)\n","    #     f.write(line + '\\n')\n","    # f = open(os.path.join(data_dir, \"negative_cleaned.txt\"), 'w', encoding='utf-8')\n","    # for sentence in neg_cleaned_data:\n","    #     line = ' '.join(sentence)\n","    #     f.write(line + '\\n')\n","    # f.close()\n","    # # indexed_data = word_to_index(d, word2idx)\n","    # #\n","    f = open(os.path.join(constant.data_dir, \"preprocessed\\\\train\\\\positive_cleaned.txt\"), 'r', encoding='utf-8')\n","    pos_cleaned_data = f.read()\n","    pos_cleaned_data = pos_cleaned_data.split('\\n')\n","    f = open(os.path.join(constant.data_dir, \"preprocessed\\\\train\\\\negative_cleaned.txt\"), 'r', encoding='utf-8')\n","    neg_cleaned_data = f.read()\n","    neg_cleaned_data = neg_cleaned_data.split('\\n')\n","    pos_cleaned_data = [sentence.split(' ') for sentence in pos_cleaned_data]\n","    neg_cleaned_data = [sentence.split(' ') for sentence in neg_cleaned_data]\n","    word2index(neg_cleaned_data + pos_cleaned_data)\n","    # add_labels(positive_list=pos_cleaned_data, negative_list=neg_cleaned_data)\n","    # shuffle_file()\n","    print('done')\n","    # return neg_data, pos_data\n","    return pos_cleaned_data, neg_cleaned_data\n","\n","\n","def get_word_index():\n","    with open(os.path.join(constant.data_dir, constant.WORD2INDEX), encoding='utf-8') as json_file:\n","        word2idx = json.load(json_file)\n","    return word2idx\n","\n","\n","def load_cleaned_data(file_path: str):\n","    f = open(file_path, 'r', encoding='utf-8')\n","    df = f.read()\n","    df = df.split('\\n')\n","    df = [sentence.split('| ') for sentence in df]\n","    df = df[:-1]\n","    x = [item[0] for item in df]\n","    y = [int(item[1]) for item in df]\n","    return x, y\n","\n","\n","def load_data(flag):\n","   # path = os.path.join(data_dir, 'preprocessed')\n","    if flag == TRAINING_DATA:\n","        #path = os.path.join(path, \"train\\\\shuffled_data.txt\")\n","        path = 'shuffled_data_train.txt'\n","    if flag == TESTING_DATA:\n","        #path = os.path.join(path, \"test\\\\shuffled_data.txt\")\n","         path = 'shuffled_data_test.txt'\n","    if flag == VALIDATION_DATA:\n","       # path = os.path.join(path, \"validation\\\\shuffled_data.txt\")\n","         path = 'shuffled_data_validation.txt'\n","\n","    x, y = load_cleaned_data(path)\n","    x = [sentence.split(' ') for sentence in x]\n","    with open(WORD2INDEX, encoding='utf-8') as json_file:\n","        word2idx = json.load(json_file)\n","    x = word_to_index(x, word2idx)\n","    return x, y\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dIEKFAn8Gkf7","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"sRTQTIYLGlEj","colab_type":"code","outputId":"bb8f74cd-ad20-48bd-88ea-9e8a6abc3dfc","executionInfo":{"status":"ok","timestamp":1557061613265,"user_tz":-420,"elapsed":13414,"user":{"displayName":"Pham Hung Viet","photoUrl":"https://lh3.googleusercontent.com/-ejMvZknjTC0/AAAAAAAAAAI/AAAAAAAAArU/Hk2HcSc58O8/s64/photo.jpg","userId":"13826574879495788675"}},"colab":{"base_uri":"https://localhost:8080/","height":373}},"source":["pip install pyvi"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting pyvi\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/6d/09e335f8399507cfa2260d5f06b27e6a9399a79251101fa7a47e5d294029/pyvi-0.0.9.3.tar.gz (5.2MB)\n","\u001b[K     |████████████████████████████████| 5.3MB 3.1MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyvi) (0.20.3)\n","Collecting sklearn-crfsuite (from pyvi)\n","  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.16.3)\n","Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.2.1)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (4.28.1)\n","Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->pyvi)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/86/cfcd71edca9d25d3d331209a20f6314b6f3f134c29478f90559cee9ce091/python_crfsuite-0.9.6-cp36-cp36m-manylinux1_x86_64.whl (754kB)\n","\u001b[K     |████████████████████████████████| 757kB 40.3MB/s \n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (0.8.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (1.12.0)\n","Building wheels for collected packages: pyvi\n","  Building wheel for pyvi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/49/44/c1/56344a2e33862991f04fdbacc8b8369bfc597723e63cdf17ea\n","Successfully built pyvi\n","Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n","Successfully installed python-crfsuite-0.9.6 pyvi-0.0.9.3 sklearn-crfsuite-0.3.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GQ2Iv68jRTtN","colab_type":"code","outputId":"2b5249eb-f17b-498d-d737-3707b7ef20e1","executionInfo":{"status":"error","timestamp":1557066786883,"user_tz":-420,"elapsed":16657,"user":{"displayName":"Pham Hung Viet","photoUrl":"https://lh3.googleusercontent.com/-ejMvZknjTC0/AAAAAAAAAAI/AAAAAAAAArU/Hk2HcSc58O8/s64/photo.jpg","userId":"13826574879495788675"}},"colab":{"base_uri":"https://localhost:8080/","height":740}},"source":["import keras as K\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, Dropout\n","import os\n","import time\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","def recall(y_true, y_pred):\n","    \"\"\"Recall metric.\n","\n","    Only computes a batch-wise average of recall.\n","\n","    Computes the recall, a metric for multi-label classification of\n","    how many relevant items are selected.\n","    \"\"\"\n","    true_positives = K.backend.sum(K.backend.round(K.backend.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.backend.sum(K.backend.round(K.backend.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.backend.epsilon())\n","    return recall\n","\n","\n","def precision(y_true, y_pred):\n","    \"\"\"Precision metric.\n","\n","    Only computes a batch-wise average of precision.\n","\n","    Computes the precision, a metric for multi-label classification of\n","    how many selected items are relevant.\n","    \"\"\"\n","    true_positives = K.backend.sum(K.backend.round(K.backend.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.backend.sum(K.backend.round(K.backend.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.backend.epsilon())\n","    return precision\n","\n","\n","def f1(y_true, y_pred):\n","    def recall(y_true, y_pred):\n","        \"\"\"Recall metric.\n","\n","        Only computes a batch-wise average of recall.\n","\n","        Computes the recall, a metric for multi-label classification of\n","        how many relevant items are selected.\n","        \"\"\"\n","        true_positives = K.backend.sum(K.backend.round(K.backend.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = K.backend.sum(K.backend.round(K.backend.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + K.backend.epsilon())\n","        return recall\n","\n","    def precision(y_true, y_pred):\n","        \"\"\"Precision metric.\n","\n","        Only computes a batch-wise average of precision.\n","\n","        Computes the precision, a metric for multi-label classification of\n","        how many selected items are relevant.\n","        \"\"\"\n","        true_positives = K.backend.sum(K.backend.round(K.backend.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.backend.sum(K.backend.round(K.backend.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.backend.epsilon())\n","        return precision\n","    precision = precision(y_true, y_pred)\n","    recall = recall(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.backend.epsilon()))\n","\n","\n","class SAV_Model:\n","\n","    def __init__(self, activation, max_words=20000, lstm_unit=512, output_dim=32, drop_out=0.2,\n","                 optimizer='rmsprop', loss_func='binary_crossentropy'):\n","        self.model = Sequential()\n","        self.model.add(Embedding(input_dim=max_words, output_dim=256, mask_zero=True))\n","        self.model.add(LSTM(units=lstm_unit, dropout=drop_out, recurrent_dropout=0.2))\n","        self.model.add(Dense(256, activation=activation[0]))\n","        self.model.add(Dropout(drop_out))\n","        self.model.add(Dense(1, activation=activation[1]))\n","        self.model.compile(optimizer=optimizer, loss=loss_func, metrics=[f1,recall,precision,'accuracy'])\n","\n","    def summary(self):\n","        self.model.summary()\n","\n","    def train(self, x_train, y_train, batch_size=100, n_epochs=3, verbose=1):\n","        self.model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=verbose)\n","\n","    def save_model(self, file_name):\n","        model_dir = os.path.join(file_name)\n","        self.model.save(model_dir)\n","        print(\"model is saved!!!\")\n","\n","    @staticmethod\n","    def load_model(model_path):\n","        return K.models.load_model(model_path)\n","\n","    def evaluate_model(self, x_valid, y_valid):\n","        loss_acc = self.model.evaluate(x_valid, y_valid, verbose=0)\n","        print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1] * 100))\n","        return loss_acc\n","\n","    def predict_(self, sentences: list):\n","        \"\"\"\n","        :param sentences: the sentences list will be predicted.\n","        :return: probability of positive sentiment.\n","        \"\"\"\n","        prob = []\n","        d = preprocessing.get_word_index()\n","        for sentence in sentences:\n","            review = preprocessing.clean_data(sentence)\n","            review = preprocessing.word_to_index(review, d)\n","            review = pad_sequences(review, truncating='post', padding='post', maxlen=80)\n","            prediction = self.model.predict(review)\n","            prob.append(prediction[0][0])\n","\n","        return prob\n","\n","\n","if __name__ == '__main__':\n","    x_train, y_train = load_data(TRAINING_DATA)\n","    #x_test, y_test = load_data(TESTING_DATA)\n","    x_valid, y_valid = load_data(VALIDATION_DATA)\n","   # x_train = x_train[:1000]\n","   # y_train = y_train[:1000]\n","   # x_valid = x_valid[:300]\n","   # y_valid = y_valid[:300]\n","    print('load data done')\n","    max_review_len = max([len(s) for s in x_train])\n","    x_train = pad_sequences(x_train, maxlen=max_review_len, padding='post',\n","                                                         truncating='post')\n","    x_valid = pad_sequences(x_valid, maxlen=max_review_len, padding='post',\n","                                                         truncating='post')\n","\n","    model = SAV_Model(activation=[\"tanh\", 'sigmoid'])\n","    model.summary()\n","    start = time.time()\n","    # model = train(model, x_train, y_train,n_epochs=3)\n","    model.train(x_train=x_train, y_train=y_train,n_epochs = 10)\n","    end = time.time()\n","    print('training time: %.4f seconds' % (end - start))\n","    model.save_model(\"model_lstm_new_0405.h5\")\n","    model.evaluate_model(x_valid, y_valid)\n","\n","\n","# load model and test\n","#     sentence = \"Quán nấu ăn ngon, KHÔNG GIAN chật.\"\n","#     sentence2 = \"T7 mình ghé đây. Nhân viên không nhiệt tính.\\nMang beer ra rồi để mình ngồi 1 đống. Rót beer thì đổ. \\nThật không chuyên nghiệp .\"\n","#     sentence2 = \"trái_cây nhỏ_xíu . bể nát . . loai rẻ tien . nho chua_lè . thanh_long chua_lè . dừa bào mỏng lét .\\n ăn xong 2mej con bi tào tháo dí luon . 1to.30k.bằng tô phở rui . kg dám quay lai luon . goi xe thì xa\"\n","#     d = preprocessing.get_word_index()\n","#\n","#     #\n","#     review = preprocessing.clean_data([sentence2])\n","#     # print(review)\n","#     review = preprocessing.word_to_index(review, d)\n","#     # print(review)\n","#     review = K.preprocessing.sequence.pad_sequences(review, truncating='post', padding='post', maxlen=80)\n","#     path = os.path.join(constant.MODEL_DIR, \"model_lstm_new_2704.h5\")\n","#     model = SAV_Model.load_model(model_path=path)\n","#     probs = model.predict(review)\n","#\n","#     print(\"Sentence: \\n\\t\"\"%s\"\"\" % sentence2)\n","#     print(\"Predict:\", end=\" \")\n","#     if probs[0][0] >= 0.5:\n","#         print(\"positive (prob=%0.4f)\" % probs[0][0])\n","#     else:\n","#         print(\"negative (prob=%0.4f)\" % (1 - probs[0][0]))\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["load data done\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, None, 256)         5120000   \n","_________________________________________________________________\n","lstm_4 (LSTM)                (None, 512)               1574912   \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 256)               131328    \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 6,826,497\n","Trainable params: 6,826,497\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","1000/1000 [==============================] - 8s 8ms/step - loss: 0.6667 - f1: 0.5672 - recall: 0.6715 - precision: 0.5195 - acc: 0.5780\n","Epoch 2/10\n"," 400/1000 [===========>..................] - ETA: 3s - loss: 0.5382 - f1: 0.7523 - recall: 0.6738 - precision: 0.8774 - acc: 0.7975"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-688f2b07cf3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# model = train(model, x_train, y_train,n_epochs=3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training time: %.4f seconds'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-688f2b07cf3a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, batch_size, n_epochs, verbose)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"kbnYJPcASuVk","colab_type":"code","outputId":"29a542e5-5c9e-42fd-a464-0067bd5efcee","executionInfo":{"status":"ok","timestamp":1557066818874,"user_tz":-420,"elapsed":4155,"user":{"displayName":"Pham Hung Viet","photoUrl":"https://lh3.googleusercontent.com/-ejMvZknjTC0/AAAAAAAAAAI/AAAAAAAAArU/Hk2HcSc58O8/s64/photo.jpg","userId":"13826574879495788675"}},"colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["pip install matplotlib"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.16.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c3MD_SBKRaaU","colab_type":"text"},"source":["# New Section"]}]}